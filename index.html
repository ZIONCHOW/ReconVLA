<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/eye.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img src="static/images/eye.ico" alt="icon" style="height: 1em; vertical-align: -0.15em; margin-right: .3em;">ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Wenxuan Song</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Ziyang Zhou</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Han Zhao</a><sup>2,3</sup>,</span>
                    <span class="author-block">
                      <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jiayi Chen</a><sup>1</sup>,</span>
                      <span class="author-block">
                        <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yuxin Huang</a><sup>1</sup>,</span>
                        <span class="author-block">
                          <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Pengxiang Ding</a><sup>2,3</sup>,</span>
                          <span class="author-block">
                            <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Feilong Tang</a><sup>4</sup>,</span>
                                <span class="author-block">
                                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Donglin Wang</a><sup>2</sup>,</span>
                                  <span class="author-block">
                                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Haoang Li</a><sup>1</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>The Hong Kong University of Science and Technology (Guangzhou)<br><sup>2</sup>Westlake University <sup>3</sup>Zhejiang University <sup>4</sup>Monash University</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>



                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Chowzy069/Reconvla" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in Vision-Language-Action (VLA) models have enabled robotic agents to integrate multimodal understanding with action execution.
            However, our empirical analysis reveals that current VLAs struggle to allocate visual attention to target regions.
            Instead, visual attention is always dispersed.
            To guide the visual attention grounding on the correct target, we propose \textbf{\method}, a reconstructive VLA model with an implicit grounding paradigm. 
            Conditioned on the model's visual outputs, a diffusion transformer aims to reconstruct the gaze region of the image, which corresponds to the target manipulated objects.
            This process prompts the VLA model to learn fine-grained representations and accurately allocate visual attention, thus effectively leveraging task-specific visual information and conducting precise manipulation.
            Moreover, we curate a large-scale pretraining dataset comprising over 100k trajectories and 2 million data samples from open-source robotic datasets, further boosting the modelâ€™s generalization in visual reconstruction.
            Extensive experiments in simulation and the real world demonstrate the superiority of our implicit grounding method, showcasing its capabilities of precise manipulation and generalization.nsectetur adipiscing elit. Proin ullamcorper tellus sed ante aliquam tempus. Etiam porttitor urna feugiat nibh elementum, et tempor dolor mattis. Donec accumsan enim augue, a vulputate nisi sodales sit amet. Proin bibendum ex eget mauris cursus euismod nec et nibh. Maecenas ac gravida ante, nec cursus dui. Vivamus purus nibh, placerat ac purus eget, sagittis vestibulum metus. Sed vestibulum bibendum lectus gravida commodo. Pellentesque auctor leo vitae sagittis suscipit.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Paper introduction -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduce</h2>
        <div class="content has-text-justified">
          <figure class="image is-16by4" >
            <img src="static/images/teaser_new.jpg" alt="ReconVLA architecture"  style="width:100% ; height:100% ;">
          </figure>
          <ol>
            <li> We propose ReconVLA, a reconstructive VLA model with an implicit grounding paradigm. The reconstruction of gaze regions prompts the model toward precise visual attention allocation and fine-grained representation learning, thereby enhancing visual grounding capabilities and executing precise manipulation.</li>
            <li> We constructed a large-scale robot pretraining dataset, containing more than 100k trajectories, 2 million data samples. Pretraining on this dataset enhances the model's generalization of visual reconstruction capabilities.</li>
            <li> Extensive experiments in simulation and the real world show the superiority of our implicit grounding methods and the capabilities of precise manipulation and generalization for unseen targets.</li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper introduction -->

<!-- Paper method -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <figure class="image is-16by4" >
            <img src="static/images/arch.jpg" alt="ReconVLA architecture"  style="width:100% ; height:100% ;">
          </figure>
          <ol>
            Our model consists of a reconstructive part and an action part.
            The input includes multi-view images and a text instruction.
            For the action part, the model outputs discrete action tokens.
            For the reconstruction part, ReconVLA is guided to output reconstructive tokens, which are conditions of the denoising process to reconstruct the scene tokens z<sub>0</sub> from noisy z<sub>t</sub>.
            The scene tokens are tokenized images of gaze regions.
            This supervision enables ReconVLA to enhance visual grounding and fine-grained comprehension capabilities, which contribute to precise manipulation.
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper method -->

<!-- Paper pretraining -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Visual Pretraining</h2>
        <div class="content has-text-justified">
          <figure class="image is-16by4" >
            <img src="static/images/pretrain.png" alt="ReconVLA architecture"  style="width:100% ; height:100% ;">
          </figure>
          <ol>
            To enhance its ability to ground and reconstruct specific regions, we design a pretraining process for reconstruction tasks on a large-scale robot dataset.
            We constructed the pre-training dataset based on large-scale open-source robotic datasets <a href="https://rail.eecs.berkeley.edu/datasets/bridge_release/data/" target="_blank" rel="noopener noreferrer">BridgeData V2</a>, along with high-quality simulation datasets <a href="https://libero-project.github.io/datasets" target="_blank" rel="noopener noreferrer">LIBERO</a> and <a href="https://github.com/mees/calvin" target="_blank" rel="noopener noreferrer">CALVIN</a>.
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper pretraining -->

<!-- Paper Experiments -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <figure class="image is-16by4" >
            <img src="static/images/main_epx.png" alt="ReconVLA architecture"  style="width:100% ; height:100% ;">
          </figure>
          <ol class="has-text-centered" style="font-size: 1.2rem;">
            Main experiment in simulation.
          </ol>
          <h3 class="title is-4  has-text-centered">Evaluation on CALVIN ABC->D Benchmark</h3>
          <div class="content has-text-justified">
            <video poster="" id="tree" autoplay controls muted loop height="100%">
              <!-- Your video here -->
              <source src="static/videos/111.mp4"
              type="video/mp4">
            </video>
          </div>
          <h3 class="title is-4  has-text-centered">Comparison with other methods</h3>
          <div class="content has-text-justified">
            <figure class="image is-16by4" >
              <img src="static/images/compare_mix.png" alt="ReconVLA architecture"  style="width:100% ; height:100% ;">
            </figure>
            <ol class="has-text-centered" style="font-size: 1.2rem;">
              Our implicit grounding method gets the highest success rates, which demonstrates the superiority of our method over other paradigms. 
            </ol>
          <h3 class="title is-4  has-text-centered">Ablation Study</h3>
          <div class="content has-text-justified">
            <figure class="image is-16by4" >
              <img src="static/images/Ablation.png" alt="ReconVLA architecture"  style="width:100% ; height:100% ;">
            </figure>
            <ol class="has-text-centered" style="font-size: 1.2rem;">
              We observe that pretraining leads to a significant improvement in success rates.
This is because, in unseen test environments, grounding the target object and performing reconstruction is inherently challenging and poses a generalization challenge to the modelâ€™s generative capability.
Pretraining on large-scale datasets substantially enhances the modelâ€™s generalization ability during visual reconstruction.
            </ol>
          </div>
          <h3 class="title is-4  has-text-centered">Evaluation on Real World</h3>
          <div class="content has-text-justified">
            <figure class="image is-16by4" >
              <img src="static/images/real_world_arxiv.jpg" alt="ReconVLA architecture"  style="width:100% ; height:100% ;">
            </figure>
            <ol class="has-text-centered" style="font-size: 1.2rem;">
              We evaluate the modelâ€™s generalization ability on real-world tasks.
            </ol>
            <figure class="image is-16by4" >
              <img src="static/images/real_world_basic.png" alt="ReconVLA architecture"  style="width:100% ; height:100% ;">
            </figure>
            <ol class="has-text-centered" style="font-size: 1.2rem;">
              The real world results compare with other methods.
            </ol>
          </div>
          <h3 class="title is-4  has-text-centered">Reconstruction Visualization</h3>
          <div class="content has-text-justified">
            <figure class="image is-16by4" >
              <img src="static/images/visual_1.png" alt="ReconVLA architecture"  style="width:100% ; height:100% ;">
            </figure>
            <figure class="image is-16by4" >
              <img src="static/images/visual_2.png" alt="ReconVLA architecture"  style="width:100% ; height:100% ;">
            </figure>

        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper experiments -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{,
          title={ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver},
          author={Wenxuan Song, Ziyang Zhou, Han Zhao, Jiayi Chen, Pengxiang Ding, Yuxin Huang, Feilong Tang, Donglin Wang, Haoang Li},
          journal={},
          year={2025}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
